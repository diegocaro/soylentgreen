{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "# Object Detection in Surveillance Videos\n",
    "\n",
    "This notebook demonstrates how to use the Detector class from our project to identify objects in videos from an Aqara security camera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import threading\n",
    "from queue import Queue\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML, clear_output\n",
    "\n",
    "import torch\n",
    "from PIL import Image as PILImage\n",
    "from torchvision import transforms\n",
    "\n",
    "from aqara_video.core.factory import TimelineFactory\n",
    "from aqara_video.core.clip import Clip\n",
    "from aqara_video.cli.video_loop import VideoLoop\n",
    "from aqara_video.ml.detector import Detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Setup the Object Detector\n",
    "\n",
    "We'll use the `Detector` class from our project, which uses a pre-trained Faster R-CNN model with a MobileNet backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "The detector can recognize 91 objects:\n",
      "__background__, person, bicycle, car, motorcycle, airplane, bus, train, truck, boat, traffic light, fire hydrant, N/A, stop sign, parking meter, bench, bird, cat, dog, horse...\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize the detector with a batch size of 1 for single frame processing\n",
    "detector = Detector(device=device, batch_size=1)\n",
    "\n",
    "# Display the available object categories\n",
    "print(f\"The detector can recognize {len(detector.labels)} objects:\")\n",
    "print(\", \".join(detector.labels[:20]) + \"...\" if len(detector.labels) > 20 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enhanced Detector Features\n",
    "\n",
    "Our `Detector` class now includes performance optimizations for faster inference:\n",
    "\n",
    "1. **Batch Processing**: Can process multiple frames at once for better throughput\n",
    "2. **Optimized Preprocessing**: Direct BGR to RGB conversion without PIL intermediary\n",
    "3. **Memory Efficiency**: Pre-allocated transform operations and tensor handling\n",
    "4. **Hardware Acceleration**: Proper utilization of GPU when available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Create a VideoLoop for bounding box drawing\n",
    "\n",
    "We'll use the VideoLoop class from our project to draw bounding boxes on detected objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "# Create a VideoLoop instance just to use its drawing functions\n",
    "video_loop = VideoLoop(video_producer=None)\n",
    "\n",
    "\n",
    "# Helper function to process a frame with the detector and draw bounding boxes\n",
    "def process_frame(frame, threshold=0.5):\n",
    "    # Preprocess the frame for the model\n",
    "    tensor = detector.preprocess(frame)\n",
    "\n",
    "    # Make predictions\n",
    "    predictions = detector.predict(tensor)\n",
    "\n",
    "    # Draw bounding boxes on a copy of the frame\n",
    "    result_frame = frame.copy()\n",
    "    result_frame = video_loop.draw_boxes(result_frame, predictions, threshold=threshold)\n",
    "\n",
    "    return result_frame, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Load Timeline and Clips\n",
    "\n",
    "Now let's load a timeline from your camera directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera ID: lumi1.54ef44603857\n",
      "Number of clips: 33516\n",
      "Available dates: [datetime.date(2025, 2, 6), datetime.date(2025, 2, 7), datetime.date(2025, 2, 8), datetime.date(2025, 2, 9), datetime.date(2025, 2, 10)]...\n"
     ]
    }
   ],
   "source": [
    "# Set the path to your camera directory\n",
    "# Update this path to point to your Aqara camera directory\n",
    "# CAMERA_DIR = Path(\"/mnt/hdd/diegocaro/aqara_video/lumi1.54ef44457bc9\")\n",
    "CAMERA_DIR = Path(\"/mnt/hdd/diegocaro/aqara_video/lumi1.54ef44603857\")\n",
    "\n",
    "# Create a timeline\n",
    "timeline = TimelineFactory.create_timeline(CAMERA_DIR)\n",
    "print(f\"Camera ID: {timeline.camera_id}\")\n",
    "print(f\"Number of clips: {len(timeline)}\")\n",
    "\n",
    "# Get available dates\n",
    "dates = timeline.get_available_dates()\n",
    "print(f\"Available dates: {dates[:5]}{'...' if len(dates) > 5 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Create UI Controls for Selecting Clips and Configuration\n",
    "\n",
    "Let's create UI elements to select a date and clip to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9df45a74634c4322b7e3ccb00cfb3e98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Dropdown(description='Date:', options=(('2025-02-06', datetime.date(2025, 2, 6)), ('2025-02-07'â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create widgets for date and clip selection\n",
    "date_dropdown = widgets.Dropdown(\n",
    "    options=[(str(date), date) for date in dates],\n",
    "    description=\"Date:\",\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "# Will be populated after date selection\n",
    "clip_dropdown = widgets.Dropdown(\n",
    "    options=[],\n",
    "    description=\"Clip:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "# Confidence threshold slider\n",
    "threshold_slider = widgets.FloatSlider(\n",
    "    value=0.5,\n",
    "    min=0.1,\n",
    "    max=0.9,\n",
    "    step=0.05,\n",
    "    description=\"Threshold:\",\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    "    readout_format=\".2f\",\n",
    ")\n",
    "\n",
    "# Batch size slider for optimized detector\n",
    "batch_size_slider = widgets.IntSlider(\n",
    "    value=4,\n",
    "    min=1,\n",
    "    max=16,\n",
    "    step=1,\n",
    "    description=\"Batch Size:\",\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    ")\n",
    "\n",
    "# Frame sampling rate slider\n",
    "sample_rate_slider = widgets.IntSlider(\n",
    "    value=2,\n",
    "    min=1,\n",
    "    max=10,\n",
    "    step=1,\n",
    "    description=\"Sample every n frames:\",\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    ")\n",
    "\n",
    "# Processing status text\n",
    "status_text = widgets.HTML(value=\"\")\n",
    "\n",
    "\n",
    "# Function to update clips dropdown when date changes\n",
    "def on_date_change(change):\n",
    "    if change[\"type\"] == \"change\" and change[\"name\"] == \"value\":\n",
    "        selected_date = change[\"new\"]\n",
    "        # Filter clips for the selected date\n",
    "        date_clips = [\n",
    "            clip for clip in timeline.clips if clip.timestamp.date() == selected_date\n",
    "        ]\n",
    "        # Sort by timestamp\n",
    "        date_clips.sort(key=lambda clip: clip.timestamp)\n",
    "        # Update dropdown options\n",
    "        clip_dropdown.options = [\n",
    "            (clip.timestamp.strftime(\"%H:%M:%S\"), clip) for clip in date_clips\n",
    "        ]\n",
    "        clip_dropdown.disabled = False\n",
    "\n",
    "\n",
    "# Register the callback\n",
    "date_dropdown.observe(on_date_change)\n",
    "\n",
    "# Display the widgets\n",
    "display(\n",
    "    widgets.VBox(\n",
    "        [\n",
    "            date_dropdown,\n",
    "            clip_dropdown,\n",
    "            widgets.HBox([threshold_slider, batch_size_slider, sample_rate_slider]),\n",
    "            status_text,\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Process and Display Video with Optimized Object Detection\n",
    "\n",
    "Now let's create functions to process and display a selected clip with our enhanced batch processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "language": "python"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d259071f3fea4faab27eec6fbdca398d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Image(value=b'', format='jpeg', height='600', width='800')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af78a52ebd0740f3a7d6a3a7e4ead337",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Process with Batching', icon='play', style=ButtonStâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create an image widget for displaying video frames\n",
    "output_widget = widgets.Image(format=\"jpeg\", width=800, height=600)\n",
    "display(output_widget)\n",
    "\n",
    "# Global variables for video processing\n",
    "processing_thread = None\n",
    "stop_processing = threading.Event()\n",
    "\n",
    "\n",
    "def process_clip_batch(clip, threshold=0.5, batch_size=4, sample_rate=2):\n",
    "    \"\"\"Process a clip using batch processing capabilities\"\"\"\n",
    "    status_text.value = f\"<b>Processing clip {clip.timestamp} with batch size {batch_size}, sampling every {sample_rate} frames</b>\"\n",
    "    stop_processing.clear()\n",
    "\n",
    "    # Create detector with specified batch size\n",
    "    batch_detector = Detector(device=device, batch_size=batch_size)\n",
    "\n",
    "    # Process in a separate thread\n",
    "    def process():\n",
    "        # Store processed frames and their results for display\n",
    "        processed_frames = {}\n",
    "        prediction_results = {}\n",
    "\n",
    "        # Collect frames for batch processing\n",
    "        batch_frames = []\n",
    "        batch_frame_ids = []\n",
    "\n",
    "        for frame_id, frame in clip.frames():\n",
    "            if stop_processing.is_set():\n",
    "                break\n",
    "\n",
    "            # Only process every n frames\n",
    "            if frame_id % sample_rate != 0:\n",
    "                continue\n",
    "\n",
    "            # Store the original frame\n",
    "            processed_frames[frame_id] = frame.copy()\n",
    "\n",
    "            # Add to batch for processing\n",
    "            batch_frames.append(frame)\n",
    "            batch_frame_ids.append(frame_id)\n",
    "\n",
    "            # Process batch when it reaches the target size\n",
    "            if len(batch_frames) >= batch_size:\n",
    "                batch_results = batch_detector.predict_batch(\n",
    "                    batch_frame_ids, batch_frames\n",
    "                )\n",
    "\n",
    "                # Update prediction results with batch results\n",
    "                for res_frame_id, predictions in batch_results:\n",
    "                    prediction_results[res_frame_id] = predictions\n",
    "\n",
    "                    # Update display with the processed frame\n",
    "                    display_processed_frame(\n",
    "                        res_frame_id, predictions, processed_frames, threshold\n",
    "                    )\n",
    "\n",
    "                # Clear batch buffers after processing\n",
    "                batch_frames = []\n",
    "                batch_frame_ids = []\n",
    "\n",
    "        # Process any remaining frames in the batch\n",
    "        if batch_frames:\n",
    "            batch_results = batch_detector.predict_batch(batch_frame_ids, batch_frames)\n",
    "            for res_frame_id, predictions in batch_results:\n",
    "                prediction_results[res_frame_id] = predictions\n",
    "                display_processed_frame(\n",
    "                    res_frame_id, predictions, processed_frames, threshold\n",
    "                )\n",
    "\n",
    "        status_text.value = f\"<b>Completed processing clip {clip.timestamp}</b>\"\n",
    "\n",
    "    def display_processed_frame(frame_id, predictions, frames_dict, threshold):\n",
    "        \"\"\"Helper function to display a processed frame with detections\"\"\"\n",
    "        if frame_id in frames_dict:\n",
    "            result_frame = frames_dict[frame_id].copy()\n",
    "            result_frame = video_loop.draw_boxes(\n",
    "                result_frame, predictions, threshold=threshold\n",
    "            )\n",
    "\n",
    "            # Count detections above threshold\n",
    "            detection_count = 0\n",
    "            if len(predictions) > 0:\n",
    "                scores = predictions[0][\"scores\"]\n",
    "                detection_count = sum(1 for score in scores if score > threshold)\n",
    "\n",
    "            # Update status with frame info\n",
    "            status_text.value = (\n",
    "                f\"<b>Processed frame {frame_id} - Found {detection_count} objects</b>\"\n",
    "            )\n",
    "\n",
    "            # Convert to JPEG for display\n",
    "            _, jpeg_data = cv2.imencode(\".jpg\", result_frame)\n",
    "            output_widget.value = jpeg_data.tobytes()\n",
    "\n",
    "    return threading.Thread(target=process, daemon=True)\n",
    "\n",
    "\n",
    "# Process button for batch processing\n",
    "process_button = widgets.Button(\n",
    "    description=\"Process with Batching\",\n",
    "    disabled=False,\n",
    "    button_style=\"success\",\n",
    "    tooltip=\"Process the selected clip using batch processing\",\n",
    "    icon=\"play\",\n",
    ")\n",
    "\n",
    "# Process button (single frame method)\n",
    "process_button_single = widgets.Button(\n",
    "    description=\"Process Single Frames\",\n",
    "    disabled=False,\n",
    "    button_style=\"warning\",\n",
    "    tooltip=\"Process the selected clip one frame at a time\",\n",
    "    icon=\"play\",\n",
    ")\n",
    "\n",
    "# Stop button\n",
    "stop_button = widgets.Button(\n",
    "    description=\"Stop Processing\",\n",
    "    disabled=False,\n",
    "    button_style=\"danger\",\n",
    "    tooltip=\"Stop processing\",\n",
    "    icon=\"stop\",\n",
    ")\n",
    "\n",
    "\n",
    "def process_clip_single(clip, threshold=0.5):\n",
    "    \"\"\"Process a clip and update the output widget using single frame processing\"\"\"\n",
    "    status_text.value = f\"<b>Processing clip {clip.timestamp} one frame at a time</b>\"\n",
    "    stop_processing.clear()\n",
    "\n",
    "    # Create processing function\n",
    "    def process():\n",
    "        for frame_id, frame in clip.frames():\n",
    "            if stop_processing.is_set():\n",
    "                break\n",
    "\n",
    "            # Process frame and draw bounding boxes\n",
    "            result_frame, predictions = process_frame(frame, threshold)\n",
    "\n",
    "            # Count detections above threshold\n",
    "            detection_count = 0\n",
    "            if len(predictions) > 0:\n",
    "                scores = predictions[0][\"scores\"]\n",
    "                detection_count = sum(1 for score in scores if score > threshold)\n",
    "\n",
    "            # Update status with frame info\n",
    "            status_text.value = (\n",
    "                f\"<b>Frame {frame_id} - Found {detection_count} objects</b>\"\n",
    "            )\n",
    "\n",
    "            # Convert to JPEG for display\n",
    "            _, jpeg_data = cv2.imencode(\".jpg\", result_frame)\n",
    "            # Update the image widget\n",
    "            output_widget.value = jpeg_data.tobytes()\n",
    "\n",
    "        status_text.value = f\"<b>Completed processing clip {clip.timestamp}</b>\"\n",
    "\n",
    "    return threading.Thread(target=process, daemon=True)\n",
    "\n",
    "\n",
    "def on_process_button_batch_click(b):\n",
    "    global processing_thread\n",
    "    # Stop any existing processing\n",
    "    if processing_thread and processing_thread.is_alive():\n",
    "        stop_processing.set()\n",
    "        processing_thread.join(timeout=1)\n",
    "\n",
    "    if clip_dropdown.value:\n",
    "        # Start new processing thread with batch processing\n",
    "        processing_thread = process_clip_batch(\n",
    "            clip_dropdown.value,\n",
    "            threshold_slider.value,\n",
    "            batch_size_slider.value,\n",
    "            sample_rate_slider.value,\n",
    "        )\n",
    "        processing_thread.start()\n",
    "    else:\n",
    "        status_text.value = \"<b>Please select a clip first</b>\"\n",
    "\n",
    "\n",
    "def on_process_button_single_click(b):\n",
    "    global processing_thread\n",
    "    # Stop any existing processing\n",
    "    if processing_thread and processing_thread.is_alive():\n",
    "        stop_processing.set()\n",
    "        processing_thread.join(timeout=1)\n",
    "\n",
    "    if clip_dropdown.value:\n",
    "        # Start new processing thread with original method\n",
    "        processing_thread = process_clip_single(\n",
    "            clip_dropdown.value, threshold_slider.value\n",
    "        )\n",
    "        processing_thread.start()\n",
    "    else:\n",
    "        status_text.value = \"<b>Please select a clip first</b>\"\n",
    "\n",
    "\n",
    "def on_stop_button_click(b):\n",
    "    stop_processing.set()\n",
    "    status_text.value = \"<b>Processing stopped</b>\"\n",
    "\n",
    "\n",
    "process_button.on_click(on_process_button_batch_click)\n",
    "process_button_single.on_click(on_process_button_single_click)\n",
    "stop_button.on_click(on_stop_button_click)\n",
    "\n",
    "# Display the buttons\n",
    "display(widgets.HBox([process_button, process_button_single, stop_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Benchmark Processing Speed\n",
    "\n",
    "Let's add a function to compare the processing speeds with different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebeb94a44ff443c2bd07e53d05d4577b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='info', description='Benchmark Methods', icon='dashboard', style=ButtonStylâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-6 (process):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1451378/228633817.py\", line 28, in process\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 243, in frames\n",
      "    stream = self.best_stream\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 139, in best_stream\n",
      "    return self.metadata.get_best_stream()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 128, in metadata\n",
      "    self._metadata = VideoMetadata.from_dict(ffmpeg.probe(str(self.path)))  # type: ignore\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 89, in from_dict\n",
      "    streams = [VideoStream(**stream) for stream in data.get(\"streams\", [])]\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: VideoStream.__init__() missing 15 required positional arguments: 'width', 'height', 'coded_width', 'coded_height', 'closed_captions', 'film_grain', 'has_b_frames', 'pix_fmt', 'level', 'chroma_location', 'field_order', 'refs', 'is_avc', 'nal_length_size', and 'bits_per_raw_sample'\n",
      "Exception in thread Thread-7 (process):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1451378/228633817.py\", line 28, in process\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 243, in frames\n",
      "    stream = self.best_stream\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 139, in best_stream\n",
      "    return self.metadata.get_best_stream()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 128, in metadata\n",
      "    self._metadata = VideoMetadata.from_dict(ffmpeg.probe(str(self.path)))  # type: ignore\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 89, in from_dict\n",
      "    streams = [VideoStream(**stream) for stream in data.get(\"streams\", [])]\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: VideoStream.__init__() missing 15 required positional arguments: 'width', 'height', 'coded_width', 'coded_height', 'closed_captions', 'film_grain', 'has_b_frames', 'pix_fmt', 'level', 'chroma_location', 'field_order', 'refs', 'is_avc', 'nal_length_size', and 'bits_per_raw_sample'\n",
      "Exception in thread Thread-8 (process):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1451378/228633817.py\", line 28, in process\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 243, in frames\n",
      "    stream = self.best_stream\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 139, in best_stream\n",
      "    return self.metadata.get_best_stream()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 128, in metadata\n",
      "    self._metadata = VideoMetadata.from_dict(ffmpeg.probe(str(self.path)))  # type: ignore\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 89, in from_dict\n",
      "    streams = [VideoStream(**stream) for stream in data.get(\"streams\", [])]\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: VideoStream.__init__() missing 15 required positional arguments: 'width', 'height', 'coded_width', 'coded_height', 'closed_captions', 'film_grain', 'has_b_frames', 'pix_fmt', 'level', 'chroma_location', 'field_order', 'refs', 'is_avc', 'nal_length_size', and 'bits_per_raw_sample'\n",
      "Exception in thread Thread-9 (process):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1451378/228633817.py\", line 28, in process\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 243, in frames\n",
      "    stream = self.best_stream\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 139, in best_stream\n",
      "    return self.metadata.get_best_stream()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 128, in metadata\n",
      "    self._metadata = VideoMetadata.from_dict(ffmpeg.probe(str(self.path)))  # type: ignore\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 89, in from_dict\n",
      "    streams = [VideoStream(**stream) for stream in data.get(\"streams\", [])]\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: VideoStream.__init__() missing 15 required positional arguments: 'width', 'height', 'coded_width', 'coded_height', 'closed_captions', 'film_grain', 'has_b_frames', 'pix_fmt', 'level', 'chroma_location', 'field_order', 'refs', 'is_avc', 'nal_length_size', and 'bits_per_raw_sample'\n",
      "Exception in thread Thread-10 (process):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1451378/228633817.py\", line 28, in process\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 243, in frames\n",
      "    stream = self.best_stream\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 139, in best_stream\n",
      "    return self.metadata.get_best_stream()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 128, in metadata\n",
      "    self._metadata = VideoMetadata.from_dict(ffmpeg.probe(str(self.path)))  # type: ignore\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 89, in from_dict\n",
      "    streams = [VideoStream(**stream) for stream in data.get(\"streams\", [])]\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: VideoStream.__init__() missing 15 required positional arguments: 'width', 'height', 'coded_width', 'coded_height', 'closed_captions', 'film_grain', 'has_b_frames', 'pix_fmt', 'level', 'chroma_location', 'field_order', 'refs', 'is_avc', 'nal_length_size', and 'bits_per_raw_sample'\n",
      "Exception in thread Thread-11 (process):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/threading.py\", line 1075, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 766, in run_closure\n",
      "    _threading_Thread_run(self)\n",
      "  File \"/home/diegocaro/miniconda3/envs/ds/lib/python3.12/threading.py\", line 1012, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/tmp/ipykernel_1451378/228633817.py\", line 28, in process\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 243, in frames\n",
      "    stream = self.best_stream\n",
      "             ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 139, in best_stream\n",
      "    return self.metadata.get_best_stream()\n",
      "           ^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 128, in metadata\n",
      "    self._metadata = VideoMetadata.from_dict(ffmpeg.probe(str(self.path)))  # type: ignore\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/diegocaro/notebooks/soylentgreen/aqara_video/core/video_reader.py\", line 89, in from_dict\n",
      "    streams = [VideoStream(**stream) for stream in data.get(\"streams\", [])]\n",
      "               ^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: VideoStream.__init__() missing 15 required positional arguments: 'width', 'height', 'coded_width', 'coded_height', 'closed_captions', 'film_grain', 'has_b_frames', 'pix_fmt', 'level', 'chroma_location', 'field_order', 'refs', 'is_avc', 'nal_length_size', and 'bits_per_raw_sample'\n"
     ]
    }
   ],
   "source": [
    "def benchmark_methods(\n",
    "    clip, num_frames=100, batch_sizes=[1, 2, 4, 8], sample_rates=[1, 2, 3]\n",
    "):\n",
    "    \"\"\"Benchmark different processing methods and parameters\"\"\"\n",
    "    results = []\n",
    "\n",
    "    # Single frame method first as baseline\n",
    "    status_text.value = f\"<b>Benchmarking single frame method...</b>\"\n",
    "    single_detector = Detector(device=device, batch_size=1)\n",
    "    start_time = time.time()\n",
    "    frame_count = 0\n",
    "\n",
    "    for frame_id, frame in clip.frames():\n",
    "        tensor = single_detector.preprocess(frame)\n",
    "        predictions = single_detector.predict(tensor)\n",
    "        frame_count += 1\n",
    "        if frame_count >= num_frames:\n",
    "            break\n",
    "\n",
    "    orig_time = time.time() - start_time\n",
    "    fps_orig = frame_count / orig_time\n",
    "    results.append((\"Single Frame\", 1, 1, fps_orig))\n",
    "\n",
    "    # Now benchmark batch method with different batch sizes and sample rates\n",
    "    for batch_size in batch_sizes:\n",
    "        for sample_rate in sample_rates:\n",
    "            status_text.value = f\"<b>Benchmarking batch method with size={batch_size}, sample_rate={sample_rate}...</b>\"\n",
    "            batch_detector = Detector(device=device, batch_size=batch_size)\n",
    "\n",
    "            start_time = time.time()\n",
    "            frame_count = 0\n",
    "            processed_count = 0\n",
    "\n",
    "            # Collect frames for batch processing\n",
    "            batch_frames = []\n",
    "            batch_frame_ids = []\n",
    "\n",
    "            for frame_id, frame in clip.frames():\n",
    "                frame_count += 1\n",
    "\n",
    "                # Skip frames based on sample rate\n",
    "                if frame_id % sample_rate != 0:\n",
    "                    continue\n",
    "\n",
    "                processed_count += 1\n",
    "                batch_frames.append(frame)\n",
    "                batch_frame_ids.append(frame_id)\n",
    "\n",
    "                # Process batch when it reaches the target size\n",
    "                if len(batch_frames) >= batch_size:\n",
    "                    _ = batch_detector.predict_batch(batch_frame_ids, batch_frames)\n",
    "                    batch_frames = []\n",
    "                    batch_frame_ids = []\n",
    "\n",
    "                if frame_count >= num_frames:\n",
    "                    break\n",
    "\n",
    "            # Process any remaining frames\n",
    "            if batch_frames:\n",
    "                _ = batch_detector.predict_batch(batch_frame_ids, batch_frames)\n",
    "\n",
    "            opt_time = time.time() - start_time\n",
    "            # Calculate FPS based on actual processed frames\n",
    "            fps_opt = frame_count / opt_time  # Overall FPS including skipped frames\n",
    "            effective_fps = processed_count / opt_time  # Effective processing rate\n",
    "\n",
    "            results.append(\n",
    "                (f\"Batch Size {batch_size}\", batch_size, sample_rate, fps_opt)\n",
    "            )\n",
    "\n",
    "    # Display results as HTML table\n",
    "    html = \"<h3>Benchmark Results</h3>\"\n",
    "    html += \"<table border='1'>\"\n",
    "    html += \"<tr><th>Method</th><th>Batch Size</th><th>Sample Rate</th><th>FPS</th><th>Speedup</th></tr>\"\n",
    "\n",
    "    baseline_fps = results[0][3]  # Single frame method FPS\n",
    "\n",
    "    for method, batch_size, sample_rate, fps in results:\n",
    "        speedup = fps / baseline_fps\n",
    "        html += f\"<tr><td>{method}</td><td>{batch_size}</td><td>{sample_rate}</td><td>{fps:.2f}</td><td>{speedup:.2f}x</td></tr>\"\n",
    "\n",
    "    html += \"</table>\"\n",
    "    html += \"<p><b>Note:</b> The FPS values represent total frames processed. With sample_rate > 1, this means some frames are skipped.</p>\"\n",
    "\n",
    "    status_text.value = html\n",
    "\n",
    "\n",
    "# Benchmark button\n",
    "benchmark_button = widgets.Button(\n",
    "    description=\"Benchmark Methods\",\n",
    "    disabled=False,\n",
    "    button_style=\"info\",\n",
    "    tooltip=\"Benchmark different processing methods\",\n",
    "    icon=\"dashboard\",\n",
    ")\n",
    "\n",
    "# Frame count for benchmark\n",
    "benchmark_frames = widgets.IntSlider(\n",
    "    value=50,\n",
    "    min=10,\n",
    "    max=200,\n",
    "    step=10,\n",
    "    description=\"Frames to benchmark:\",\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    ")\n",
    "\n",
    "\n",
    "def on_benchmark_button_click(b):\n",
    "    if clip_dropdown.value:\n",
    "        benchmark_methods(clip_dropdown.value, num_frames=benchmark_frames.value)\n",
    "    else:\n",
    "        status_text.value = \"<b>Please select a clip first</b>\"\n",
    "\n",
    "\n",
    "benchmark_button.on_click(on_benchmark_button_click)\n",
    "\n",
    "# Display benchmark controls\n",
    "display(widgets.HBox([benchmark_button, benchmark_frames]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clip Analysis with Batch Processing\n",
    "\n",
    "We can use batch processing to efficiently analyze a clip and count objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_clip_batch(clip, threshold=0.5, batch_size=4, sample_rate=5):\n",
    "    \"\"\"Analyze a clip using batch processing\"\"\"\n",
    "    detection_counts = {}\n",
    "    processed_frames = 0\n",
    "\n",
    "    status_text.value = f\"<b>Analyzing clip {clip.timestamp} with batch size {batch_size}, sampling every {sample_rate} frames</b>\"\n",
    "\n",
    "    # Create detector with specified batch size\n",
    "    batch_detector = Detector(device=device, batch_size=batch_size)\n",
    "\n",
    "    # Collect frames for batch processing\n",
    "    batch_frames = []\n",
    "    batch_frame_ids = []\n",
    "\n",
    "    # Process frames in batches\n",
    "    for frame_id, frame in clip.frames():\n",
    "        # Only process every N frames\n",
    "        if frame_id % sample_rate != 0:\n",
    "            continue\n",
    "\n",
    "        processed_frames += 1\n",
    "        status_text.value = f\"<b>Adding frame {frame_id} to batch</b>\"\n",
    "\n",
    "        # Add to batch for processing\n",
    "        batch_frames.append(frame)\n",
    "        batch_frame_ids.append(frame_id)\n",
    "\n",
    "        # Process batch when it reaches the target size\n",
    "        if len(batch_frames) >= batch_size:\n",
    "            batch_results = batch_detector.predict_batch(batch_frame_ids, batch_frames)\n",
    "\n",
    "            # Process detection results\n",
    "            process_detection_results(batch_results, detection_counts, threshold)\n",
    "\n",
    "            # Clear batch buffers\n",
    "            batch_frames = []\n",
    "            batch_frame_ids = []\n",
    "\n",
    "    # Process any remaining frames in the batch\n",
    "    if batch_frames:\n",
    "        batch_results = batch_detector.predict_batch(batch_frame_ids, batch_frames)\n",
    "        process_detection_results(batch_results, detection_counts, threshold)\n",
    "\n",
    "    # Helper function to process detection results\n",
    "    def process_detection_results(batch_results, detection_counts, threshold):\n",
    "        for _, predictions in batch_results:\n",
    "            if len(predictions) > 0:\n",
    "                scores = predictions[0][\"scores\"]\n",
    "                categories = predictions[0][\"categories\"]\n",
    "\n",
    "                # Count objects above threshold\n",
    "                for i, (category, score) in enumerate(zip(categories, scores)):\n",
    "                    if score > threshold:\n",
    "                        if category not in detection_counts:\n",
    "                            detection_counts[category] = 0\n",
    "                        detection_counts[category] += 1\n",
    "\n",
    "    # Sort by count (descending)\n",
    "    sorted_detections = sorted(\n",
    "        detection_counts.items(), key=lambda x: x[1], reverse=True\n",
    "    )\n",
    "\n",
    "    # Prepare results HTML\n",
    "    result_html = f\"<h3>Analysis of clip {clip.timestamp}</h3>\"\n",
    "    result_html += f\"<p>Processed {processed_frames} frames (sampling every {sample_rate} frames)</p>\"\n",
    "    result_html += \"<h4>Detected Objects:</h4>\"\n",
    "    result_html += \"<ul>\"\n",
    "    for category, count in sorted_detections:\n",
    "        result_html += f\"<li><b>{category}</b>: {count} instances</li>\"\n",
    "    result_html += \"</ul>\"\n",
    "\n",
    "    status_text.value = result_html\n",
    "\n",
    "\n",
    "# Analyze button\n",
    "analyze_button = widgets.Button(\n",
    "    description=\"Analyze Clip\",\n",
    "    disabled=False,\n",
    "    button_style=\"info\",\n",
    "    tooltip=\"Analyze the selected clip with batch processing\",\n",
    "    icon=\"search\",\n",
    ")\n",
    "\n",
    "\n",
    "def on_analyze_button_click(b):\n",
    "    if clip_dropdown.value:\n",
    "        analyze_clip_batch(\n",
    "            clip_dropdown.value,\n",
    "            threshold_slider.value,\n",
    "            batch_size_slider.value,\n",
    "            sample_rate_slider.value,\n",
    "        )\n",
    "    else:\n",
    "        status_text.value = \"<b>Please select a clip first</b>\"\n",
    "\n",
    "\n",
    "analyze_button.on_click(on_analyze_button_click)\n",
    "\n",
    "# Display the analyze button\n",
    "display(analyze_button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Process a Single Frame\n",
    "\n",
    "You can also process a single frame from a clip for detailed inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "language": "python"
   },
   "outputs": [],
   "source": [
    "def get_single_frame(clip, frame_num=0):\n",
    "    \"\"\"Get a specific frame from a clip\"\"\"\n",
    "    for i, (frame_id, frame) in enumerate(clip.frames()):\n",
    "        if i == frame_num:\n",
    "            return frame\n",
    "    return None\n",
    "\n",
    "\n",
    "frame_slider = widgets.IntSlider(\n",
    "    value=0,\n",
    "    min=0,\n",
    "    max=100,  # Will be updated when clip is selected\n",
    "    step=1,\n",
    "    description=\"Frame #:\",\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation=\"horizontal\",\n",
    "    readout=True,\n",
    ")\n",
    "\n",
    "process_frame_button = widgets.Button(\n",
    "    description=\"Process Frame\",\n",
    "    disabled=False,\n",
    "    button_style=\"primary\",\n",
    "    tooltip=\"Process the selected frame\",\n",
    "    icon=\"camera\",\n",
    ")\n",
    "\n",
    "\n",
    "def on_process_frame_button_click(b):\n",
    "    if clip_dropdown.value:\n",
    "        clip = clip_dropdown.value\n",
    "        frame = get_single_frame(clip, frame_slider.value)\n",
    "        if frame is not None:\n",
    "            result_frame, predictions = process_frame(frame, threshold_slider.value)\n",
    "\n",
    "            # Show detailed detection info\n",
    "            detection_info = \"<h4>Detections:</h4><ul>\"\n",
    "            if len(predictions) > 0:\n",
    "                boxes = predictions[0][\"boxes\"]\n",
    "                scores = predictions[0][\"scores\"]\n",
    "                categories = predictions[0][\"categories\"]\n",
    "\n",
    "                for i, (box, category, score) in enumerate(\n",
    "                    zip(boxes, categories, scores)\n",
    "                ):\n",
    "                    if score > threshold_slider.value:\n",
    "                        x1, y1, x2, y2 = box\n",
    "                        detection_info += f\"<li><b>{category}</b> (confidence: {score:.2f}) at [{x1}, {y1}, {x2}, {y2}]</li>\"\n",
    "            detection_info += \"</ul>\"\n",
    "\n",
    "            # Convert to JPEG for display\n",
    "            _, jpeg_data = cv2.imencode(\".jpg\", result_frame)\n",
    "            output_widget.value = jpeg_data.tobytes()\n",
    "\n",
    "            status_text.value = (\n",
    "                f\"<b>Frame {frame_slider.value} from clip {clip.timestamp}</b><br>\"\n",
    "                + detection_info\n",
    "            )\n",
    "        else:\n",
    "            status_text.value = \"<b>Could not retrieve the specified frame</b>\"\n",
    "    else:\n",
    "        status_text.value = \"<b>Please select a clip first</b>\"\n",
    "\n",
    "\n",
    "process_frame_button.on_click(on_process_frame_button_click)\n",
    "\n",
    "# Display frame processing controls\n",
    "display(widgets.HBox([frame_slider, process_frame_button]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "language": "markdown"
   },
   "source": [
    "## Optimization Strategies\n",
    "\n",
    "In this notebook, we've implemented several strategies to improve prediction speed without changing the ML model:\n",
    "\n",
    "1. **Batch Processing**: By processing multiple frames in a batch, we leverage PyTorch's ability to parallelize operations, which is much more efficient than processing frames one by one.\n",
    "\n",
    "2. **Frame Sampling**: Instead of processing every frame, we can skip frames (e.g., process every 2nd or 3rd frame) which drastically reduces the computational load while still providing good results for most applications.\n",
    "\n",
    "3. **Optimized Preprocessing**: We've streamlined the preprocessing steps to reduce redundant operations.\n",
    "\n",
    "4. **Memory Management**: Pre-allocating the transforms and reusing tensors where possible helps reduce memory allocation overhead.\n",
    "\n",
    "5. **Hardware Acceleration**: We ensure proper use of GPU acceleration when available.\n",
    "\n",
    "You can use the benchmark tool to compare the performance of different configurations and find the optimal settings for your specific hardware."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
